x-default-service: &default-service
  restart: unless-stopped
  env_file:
    - ${PC1_AI_ENV_FILE:-.env}
  networks:
    - pc1-ai-net
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"

services:
  mcp-glama:
    <<: *default-service
    build:
      context: ${MCP_GLAMA_BUILD_CONTEXT:-../../mcp/mcp-glama}
    container_name: pc1-ai-mcp-glama
    ports:
      - "${MCP_GLAMA_PORT:-7241}:8014"
    environment:
      PORT: 8014
      GLAMA_API_KEY: ${GLAMA_API_KEY}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8014/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3

  mcp-github-models:
    <<: *default-service
    build:
      context: ${MCP_GITHUB_MODELS_BUILD_CONTEXT:-../../mcp/mcp-github-models}
    container_name: pc1-ai-mcp-github-models
    ports:
      - "${MCP_GITHUB_MODELS_PORT:-7242}:8015"
    environment:
      PORT: 8015
      GITHUB_TOKEN: ${GITHUB_TOKEN}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8015/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3

  mcp-openai-gateway:
    <<: *default-service
    build:
      context: ${MCP_OPENAI_GATEWAY_BUILD_CONTEXT:-../../mcp/mcp-openai-gateway}
    container_name: pc1-ai-mcp-openai-gateway
    ports:
      - "${MCP_OPENAI_GATEWAY_PORT:-8181}:8181"
    environment:
      PORT: 8181
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-https://api.openai.com/v1}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8181/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3

  ollama:
    <<: *default-service
    image: ollama/ollama:latest
    container_name: pc1-ai-ollama
    ports:
      - "${OLLAMA_PORT:-11435}:11434"
    environment:
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_PORT: 11434
    volumes:
      - ollama-data:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'echo > /dev/tcp/127.0.0.1/11434' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 6

  mcp-imagen-light:
    <<: *default-service
    build:
      context: ${MCP_IMAGEN_LIGHT_BUILD_CONTEXT:-../../mcp/mcp-imagen-light}
    container_name: pc1-ai-mcp-imagen-light
    ports:
      - "${MCP_IMAGEN_LIGHT_PORT:-8020}:8020"
    environment:
      PORT: 8020
      MCP_CUDA_URL: ${MCP_CUDA_URL:-http://pc1.vpn:8057}
      IMAGEN_OUTPUT_DIR: ${IMAGEN_OUTPUT_DIR:-/app/images}
      IMAGEN_BASE_URL: ${IMAGEN_BASE_URL:-http://localhost:8020}
    volumes:
      - mcp-imagen-light-images:/app/images
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8020/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3

networks:
  pc1-ai-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.24.0.0/16

volumes:
  ollama-data:
  mcp-imagen-light-images:
